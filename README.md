# Explaining Recommender Systems via Random Path-Integration

The Path-Integration (PI) method is a model-agnostic, post-hoc approach designed to explain the reasoning behind recommendations generated by recommender systems. By integrating along a path from a baseline to the input user vector, PI generates an explanation map that illustrates each user data aspect's contribution to the recommendation output. We introduce a variant of our method, wherein the baseline is modeled as a random tensor subjected to multiple sampling iterations. This process yields a comprehensive set of explanations, enabling the selection of the most effective explanation with respect to specific metrics. 
Utilizing several counterfactual evaluation metrics, our results demonstrate the method's capability to provide insightful counterfactual explanations for various recommendation algorithms across different datasets.

## The idea of Random Path-Integration
![LXR_diagram](https://github.com/ExplainRec/PI4Rec/blob/main/PI_diagram.PNG)

## Repository

This repository hosts the code for the Path-Integration (PI) framework. 
It was evaluated using three publicly available benchmarks: MovieLens1M, a subset of the Yahoo!Music dataset, and a subset of the Pinterest dataset. 
The evaluation employed three different recommenders: Multi Layer Perceptron (MLP), Variational Auto Encoder (VAE), and Neural Collaborative Filtering (NCF). 
Hyperparameter optimization was conducted using Optuna.

## Folders

* **processed_data**: contains three subfolders, one for each dataset. In each dataset folder lies the raw data files.
* **code**: contains several code notebooks:
  - data_processing - code related to the preprocessing step for preparing the raw data to run with our models.
  - recommenders_architecture - specifies the architecture of the recommenders that were used in the paper.
  - recommenders_training - contains code related to MLP, VAE and NCF recommenders training.
  - help_functions - includes the framework's functions that are being used in all notebooks.
  - PI_functions - includes the PI method related functions that are being used in the metrics notebooks.
  - mertics_PI_base - contains code related to PI base (null user) evaluation.
  - metrics_PI_random - contains code related to the Path-Integration Randomized Users evaluation.
  - metrics_baselines - contains code related to baselines evaluation.
  - lime, MLP_SHAP_clusters, VAE_SHAP_clusters, NCF_SHAP_clusters - notebook that contains help functions for calculating the LIME & SHAP baselines

* **checkpoints**: Presently, this folder is empty.  It is the designated location for saving and loading the trained recommender's checkpoints. The checkpoints developed during our project are stored in the 'checkpoints' folder in the attached [drive](https://drive.google.com/drive/u/1/folders/1v8jZD2Ew-D4XA0k1NLxgVsHS6q4Aj-KP).
  
## Requirements

* python 3.7.13
* Pytorch 1.12.1
* wandb 0.16.3 (the package we used for monitoring the recommenders training process)

## Usage

To use this code, follow these steps:
+ Create data to work with by running the data_processing notebooks.
  - Or in order to reproduce results from the paper without running the data_processing notebook, please download all files from [here](https://drive.google.com/drive/u/1/folders/1oto5QPrhisx2A4MCwub5OUHYdZTYAQxq) from the relevant folder <dataset_name> to data_preprocessing folder according to the data set you wish to run on. 
+ On every notebook, please specify the "data_name" variable to be 'ML1M'/'Yahoo'/'Pinterest', and the "recommender_name" variable to be 'MLP'/'VAE'/'NCF'.
+ All baselines' were calculated using the 'metrics' notebook, metrics for the PI approach and its variant were calculated using the dedicated notebooks.

